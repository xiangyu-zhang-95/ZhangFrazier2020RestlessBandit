{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import numpy as np\n",
    "import time\n",
    "from gurobipy import *\n",
    "import collections\n",
    "\n",
    "class fluid_model():\n",
    "    def p(self, s, a, sprime):\n",
    "        if a == 0:\n",
    "            return s == sprime\n",
    "        if a == 1:\n",
    "            tmp_a, tmp_b = s[0], s[1]\n",
    "            tmp_c, tmp_d = sprime[0], sprime[1]\n",
    "            if tmp_a == tmp_c and tmp_b + 1 == tmp_d:\n",
    "                return 1 - (tmp_a + 1)/(tmp_a + tmp_b + 2)\n",
    "            if tmp_a + 1 == tmp_c and tmp_b == tmp_d:\n",
    "                return (tmp_a + 1)/(tmp_a + tmp_b + 2)\n",
    "            return 0\n",
    "    \n",
    "    def reward(self, s, a):\n",
    "        if a == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return (s[0] + 1)/(s[0] + s[1] + 2)\n",
    "    def __init__(self, T, alphas):\n",
    "        self.alphas = alphas\n",
    "        self.T = T\n",
    "        self.m, self.x, self.y, self.z = None, {}, {}, {}\n",
    "        self.duals = None\n",
    "        self.A, self.B, self.C = set(), set(), set()\n",
    "\n",
    "    def __LP_sol(self, T, alphas):\n",
    "        \n",
    "        time = [t for t in range(T)]\n",
    "        win = [i for i in range(T)]\n",
    "        los = [j for j in range(T)]\n",
    "        d_var, reward = multidict({(t, i, j): (i+1)/(i+j+2) \\\n",
    "                                    for t in time \\\n",
    "                                        for i in win \\\n",
    "                                            for j in los})\n",
    "        m = Model(\"MAB\")\n",
    "        n = m.addVars(time, win, los, name = \"n\")\n",
    "        x = m.addVars(time, win, los, name = \"x\")\n",
    "        \n",
    "        # set ojective\n",
    "        m.setObjective(sum(x[t, i, j] * reward[(t, i, j)] \\\n",
    "                            for t in time for i in win \\\n",
    "                                for j in los), GRB.MAXIMIZE)\n",
    "        \n",
    "        # add resource constraint\n",
    "        m.addConstrs(sum(x[t, i, j] for i in win for j in los) \\\n",
    "                        == alphas[t] for t in time)\n",
    "        \n",
    "        # add initial constraint\n",
    "        m.addConstr(n[0, 0, 0] == 1)\n",
    "        m.addConstr(sum(n[0, i, j] for i in win for j in los) == 1)\n",
    "        \n",
    "        # add x constraint\n",
    "        m.addConstrs(x[t, i, j] <= n[t, i, j] for t in time for i in win for j in los)\n",
    "        m.addConstrs(x[t, i, j] >= 0 for t in time for i in win for j in los)\n",
    "        \n",
    "        # add fluid balance\n",
    "        m.addConstrs(n[t, i, j] - x[t-1, i-1, j]*reward[(t-1, i-1, j)] - x[t-1, i, j-1]*(1-reward[(t-1, i, j-1)]) - (n[t-1, i, j] - x[t-1, i, j]) == 0 \n",
    "                    for t in range(1, T) for i in range(1, T) for j in range(1, T))\n",
    "        m.addConstrs(n[t, 0, j] - x[t-1, 0, j-1]*(1-reward[(t-1, 0, j-1)]) - (n[t-1, 0, j] - x[t-1, 0, j])== 0 \n",
    "                    for t in range(1, T) for j in range(1, T))\n",
    "        m.addConstrs(n[t, i, 0] - x[t-1, i-1, 0]*reward[(t-1, i-1, 0)] - (n[t-1, i, 0] - x[t-1, i, 0]) == 0\n",
    "                    for t in range(1, T) for i in range(1, T))\n",
    "        m.addConstrs(n[t, 0, 0] - (n[t-1, 0, 0] - x[t-1, 0, 0]) == 0 for t in range(1, T))\n",
    "        m.setParam('OutputFlag', False)\n",
    "        m.optimize()\n",
    "        print('Obj: %g' % m.objVal)\n",
    "        self.objVal = m.objVal\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def __solve(self, method=1):\n",
    "        setParam(\"Method\", method)\n",
    "        T = self.T\n",
    "        m = self.__LP_sol(self.T, self.alphas)\n",
    "        self.m = m\n",
    "        self.duals = m.PI[0: T]\n",
    "        \n",
    "        self.sorted = {t: [(a, b) for a in range(T) for b in range(T) if a + b <= t] for t in range(T)}\n",
    "        v = {}\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            advantage = {(a, b): 0 for a in range(T) for b in range(T) if a + b <= t}\n",
    "            for a in range(T):\n",
    "                for b in range(T):\n",
    "                    if a + b <= t:\n",
    "                        if t == T - 1:\n",
    "                            r_pull = self.reward((a, b), 1) - self.duals[t]\n",
    "                            r_idle = 0\n",
    "                        else:\n",
    "                            r_pull = self.reward((a, b), 1) - self.duals[t] + \\\n",
    "                                     self.p((a, b), 1, (a + 1, b)) * v[t + 1, (a + 1, b)] + \\\n",
    "                                     self.p((a, b), 1, (a, b + 1)) * v[t + 1, (a, b + 1)]\n",
    "                            r_idle = v[t + 1, (a, b)]\n",
    "                        advantage[(a, b)] = r_pull - r_idle\n",
    "                        if r_pull < r_idle:\n",
    "                            v[(t, (a, b))] = r_idle\n",
    "                            continue\n",
    "                        v[(t, (a, b))] = r_pull\n",
    "            self.sorted[t].sort(reverse=True, key=lambda x: advantage[x])\n",
    "        \n",
    "        self.z[(0, (0, 0))] = 1\n",
    "        self.pull_reward = 0\n",
    "        for t in range(T):\n",
    "            resource = self.alphas[t]\n",
    "            for a, b in self.sorted[t]:\n",
    "                self.x[(t, (a, b))] = min(self.z[(t, (a, b))], resource)\n",
    "                self.pull_reward += self.x[(t, (a, b))] * self.reward((a, b), 1)\n",
    "                self.y[(t, (a, b))] = self.z[(t, (a, b))] - self.x[(t, (a, b))]\n",
    "                resource = resource - self.x[(t, (a, b))]\n",
    "            if t == T - 1:\n",
    "                continue\n",
    "            for a, b in self.sorted[t + 1]:\n",
    "                self.z[(t + 1, (a, b))] = (self.y[(t, (a, b))] if a + b <= t else 0) + \\\n",
    "                                          a / (a + b + 1) * (self.x[(t, (a - 1, b))] if a >= 1 else 0) + \\\n",
    "                                          b / (a + b + 1) * (self.x[(t, (a, b - 1))] if b >= 1 else 0)\n",
    "    \n",
    "    def calculate_occupation_measure_and_classify_state(self, epsilon=10**(-6)):\n",
    "        self.__solve()\n",
    "        m, sol, T = self.m, dict(), self.T\n",
    "\n",
    "        for key in self.z:\n",
    "            if self.x[key] > epsilon and self.y[key] < epsilon:\n",
    "                self.A.add(key)\n",
    "                continue\n",
    "            if self.x[key] > epsilon and self.y[key] > epsilon:\n",
    "                self.B.add(key)\n",
    "                continue\n",
    "            if self.x[key] < epsilon and self.y[key] > epsilon:\n",
    "                self.C.add(key)\n",
    "                continue\n",
    "        return\n",
    "    \n",
    "    def check_degeneracy(self):\n",
    "        if sorted([key[0] for key in self.B]) == [i for i in range(self.T)] and \\\n",
    "                    abs(self.objVal - self.pull_reward) < 10**(-13):\n",
    "            print(f\"\"\"\n",
    "            Model objective: {self.pull_reward}, different from {self.objVal} (solution from LP) less than 10e-15.\n",
    "            Model is non-degenerate.\n",
    "            \"\"\")\n",
    "        else:\n",
    "            raise Exception(\"Model is degenerate.\\n\")\n",
    "\n",
    "    def calculate_diffusion_index(self, epsilon=10**(-8)):\n",
    "        self.calculate_occupation_measure_and_classify_state()\n",
    "        x, y, z, T = self.x, self.y, self.z, self.T\n",
    "        v = {(t, (a, b)): 0 for t in range(T + 1) for a in range(t + 1) for b in range(t + 1) if a + b <= t}\n",
    "        diffusion_index = {(t, (a, b)): 0 for t in range(T) for a in range(t + 1) for b in range(t + 1) if a + b <= t}\n",
    "\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            state_t = [(a, b) for a in range(t + 1) for b in range(t + 1) if a + b <= t]\n",
    "            for s in state_t:\n",
    "                s_w = (s[0] + 1, s[1]) # state after win\n",
    "                s_l = (s[0], s[1] + 1) # state after loss\n",
    "                diffusion_index[(t, s)] = self.reward(s, 1) + self.p(s, 1, s_w)*v[(t + 1, s_w)] \\\n",
    "                                          + self.p(s, 1, s_l)*v[(t + 1, s_l)] \\\n",
    "                                          - self.reward(s, 0) - self.p(s, 0, s)*v[(t + 1, s)]\n",
    "            \n",
    "            l_y_g_0 = [(diffusion_index[(t, s)], s) for s in state_t if self.x[(t, s)] > epsilon]\n",
    "\n",
    "            _, sbar = min(l_y_g_0)\n",
    "            for s in state_t:\n",
    "                if (t, s) in self.A:\n",
    "                    v[(t, s)]= diffusion_index[(t, s)] - diffusion_index[(t, sbar)] + v[(t + 1, s)]\n",
    "                else:\n",
    "                    v[(t, s)] = self.reward(s, 0) + v[(t + 1, s)]\n",
    "        self.diffusion_index = diffusion_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from numba import jit\n",
    "\n",
    "# @jit(nopython=True)\n",
    "def batch_simulation(T, K, alpha, times, obj_val, a0= 1, b0 = 1):\n",
    "    rewards = []\n",
    "    start = time.time()\n",
    "    for _ in range(times):\n",
    "        state = collections.defaultdict(int)\n",
    "        state[(a0, b0)] = K\n",
    "        reward_total = 0\n",
    "        for t in range(T):\n",
    "            # sample posterior\n",
    "            posterior_sample = []\n",
    "            for a, b in state:\n",
    "                val_list = np.random.beta(a, b, state[(a, b)])\n",
    "                for val in val_list:\n",
    "                    posterior_sample.append((val, a, b))\n",
    "            posterior_sample.sort(reverse = True)\n",
    "            \n",
    "            # choose the arms to pull\n",
    "            pull = {key: 0 for key in state}\n",
    "            for i in range(int(alpha * K)):\n",
    "                _, a, b = posterior_sample[i]\n",
    "                pull[(a, b)] += 1\n",
    "            \n",
    "            reward_total += np.sum([a / (a + b) * pull[(a, b)] for a, b in pull])\n",
    "            for a, b in pull:\n",
    "                success = np.random.binomial(pull[(a, b)], a / (a + b))\n",
    "                state[(a, b)] -= pull[(a, b)]\n",
    "                state[(a + 1, b)] += success\n",
    "                state[(a, b + 1)] += pull[(a, b)] - success\n",
    "\n",
    "        rewards.append(reward_total)\n",
    "    rewards = np.array(rewards)\n",
    "    end = time.time()\n",
    "    return obj_val * K - np.mean(rewards), K, times, np.mean(rewards), np.std(rewards), end - start\n",
    "\n",
    "def wrapper(args):\n",
    "    return batch_simulation(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_TS(T, N, alpha, n_proc, obj_val, times=1000):\n",
    "    args = (T, N, alpha, times, obj_val)\n",
    "    with Pool(n_proc) as p:\n",
    "        # res = [..., [opt_gap, N, M, mean, std, time],...]\n",
    "        res = p.map(wrapper, [args for _ in range(n_proc)])\n",
    "    m = times * n_proc\n",
    "    mean = np.mean([item[3] for item in res])\n",
    "    std = np.sqrt(np.sum([times * item[4] ** 2 for item in res])) / (n_proc * times)\n",
    "    comp_time = np.max([item[-1] for item in res])\n",
    "    opt_gap = np.mean([item[0] for item in res])\n",
    "    return opt_gap, N, m, mean, std, comp_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson_sampling_simulation(N, alpha, model, times=1000):\n",
    "    T = model.T\n",
    "    obj_val = model.objVal\n",
    "    n_proc = int(N * 50 / times) + 1\n",
    "    return parallel_TS(T, N, alpha, n_proc, obj_val, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using license file /home/xz556/gurobi.lic\n",
      "Academic license - for non-commercial use only\n",
      "Changed value of parameter Method to 1\n",
      "   Prev: -1  Min: -1  Max: 5  Default: -1\n",
      "Obj: 4.81431\n",
      "N: 150 finished. opt-gap            52.900385\n",
      "N                 150.000000\n",
      "M                8000.000000\n",
      "expect-reward     669.246389\n",
      "std                 0.201517\n",
      "time               23.702271\n",
      "Name: 150, dtype: float64\n",
      "N: 300 finished. opt-gap            107.270747\n",
      "N                  300.000000\n",
      "M                16000.000000\n",
      "expect-reward     1337.022801\n",
      "std                  0.191867\n",
      "time                28.234279\n",
      "Name: 300, dtype: float64\n",
      "N: 600 finished. opt-gap            210.848644\n",
      "N                  600.000000\n",
      "M                31000.000000\n",
      "expect-reward     2677.738452\n",
      "std                  0.195065\n",
      "time                37.137436\n",
      "Name: 600, dtype: float64\n",
      "N: 1200 finished. opt-gap            421.799200\n",
      "N                 1200.000000\n",
      "M                61000.000000\n",
      "expect-reward     5355.374992\n",
      "std                  0.196272\n",
      "time                86.455409\n",
      "Name: 1200, dtype: float64\n",
      "N: 2400 finished. opt-gap             841.265756\n",
      "N                  2400.000000\n",
      "M                121000.000000\n",
      "expect-reward     10713.082627\n",
      "std                   0.197062\n",
      "time                256.921989\n",
      "Name: 2400, dtype: float64\n",
      "N: 4800 finished. opt-gap            1685.915105\n",
      "N                  4800.000000\n",
      "M                241000.000000\n",
      "expect-reward     21422.781662\n",
      "std                   0.190374\n",
      "time               1035.753233\n",
      "Name: 4800, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "T, alpha = 20, 1/3\n",
    "model = fluid_model(T, [alpha]*T)\n",
    "model.calculate_diffusion_index()\n",
    "\n",
    "def create_file(model, file_name, start=150, end=4800):\n",
    "    if os.path.exists(file_name):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(index=[\"opt-gap\", \"N\", \"M\", \"expect-reward\", \"std\", \"time\"])\n",
    "    df.to_csv(file_name, index=True)\n",
    "    N = start\n",
    "    while N <= end:\n",
    "        res = thompson_sampling_simulation(N, alpha, model, times=1000)\n",
    "        df[N] = res\n",
    "        df.to_csv(file_name, index=True)\n",
    "        print(f\"N: {N} finished. {df[N]}\")\n",
    "        N = N * 2\n",
    "    return\n",
    "create_file(model, \"ts-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
